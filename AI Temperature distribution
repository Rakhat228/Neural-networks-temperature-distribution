{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cell_id": "e5c8300c0ac14e21b29cdbc8f68f5266",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 4583,
    "execution_start": 1672111429347,
    "source_hash": "55df9984",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cell_id": "8671736fbf014220b4c71fc5f8831f3e",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 285,
    "execution_start": 1672111433941,
    "source_hash": "c51c1c30",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>Temp1</th>\n",
       "      <th>Temp2</th>\n",
       "      <th>Temp3</th>\n",
       "      <th>Temp4</th>\n",
       "      <th>Temp5</th>\n",
       "      <th>Temp6</th>\n",
       "      <th>Temp7</th>\n",
       "      <th>Temp8</th>\n",
       "      <th>Temp9</th>\n",
       "      <th>...</th>\n",
       "      <th>c1_23l</th>\n",
       "      <th>c1_32h</th>\n",
       "      <th>c1_32l</th>\n",
       "      <th>c2_off</th>\n",
       "      <th>c2_16h</th>\n",
       "      <th>c2_16l</th>\n",
       "      <th>c2_23h</th>\n",
       "      <th>c2_23l</th>\n",
       "      <th>c2_32h</th>\n",
       "      <th>c2_32l</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-02-16 13:44:23</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>21</td>\n",
       "      <td>22</td>\n",
       "      <td>23</td>\n",
       "      <td>22</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-02-16 13:54:23</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>23</td>\n",
       "      <td>22</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-02-16 14:04:23</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>23</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>23</td>\n",
       "      <td>22</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-02-16 14:14:23</td>\n",
       "      <td>23</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-02-16 14:24:23</td>\n",
       "      <td>23</td>\n",
       "      <td>22</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Time  Temp1  Temp2  Temp3  Temp4  Temp5  Temp6  Temp7  \\\n",
       "0  2021-02-16 13:44:23     21     21     22     22     22     21     22   \n",
       "1  2021-02-16 13:54:23     21     21     22     22     22     22     22   \n",
       "2  2021-02-16 14:04:23     22     22     22     22     23     22     22   \n",
       "3  2021-02-16 14:14:23     23     22     22     23     23     22     22   \n",
       "4  2021-02-16 14:24:23     23     22     23     23     23     23     23   \n",
       "\n",
       "   Temp8  Temp9  ...  c1_23l  c1_32h  c1_32l  c2_off  c2_16h  c2_16l  c2_23h  \\\n",
       "0     23     22  ...       1       0       0       0       0       0       0   \n",
       "1     23     22  ...       1       0       0       0       0       0       0   \n",
       "2     23     22  ...       1       0       0       0       0       0       0   \n",
       "3     23     23  ...       1       0       0       0       0       0       0   \n",
       "4     23     23  ...       1       0       0       0       0       0       0   \n",
       "\n",
       "   c2_23l  c2_32h  c2_32l  \n",
       "0       1       0       0  \n",
       "1       1       0       0  \n",
       "2       1       0       0  \n",
       "3       1       0       0  \n",
       "4       1       0       0  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df= pd.read_csv(\"dataset_temperature_2.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cell_id": "6636b39082de4f58990dc62b0e511a17",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 1,
    "execution_start": 1672111434273,
    "source_hash": "30e34e49",
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df.set_index('Time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cell_id": "2cb718dce3b442c9bd44f4fec80bdc14",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 2,
    "execution_start": 1672111434274,
    "source_hash": "de1e323c",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 7904 entries, 2021-02-16 13:44:23 to 2021-04-12 10:54:46\n",
      "Data columns (total 24 columns):\n",
      " #   Column  Non-Null Count  Dtype\n",
      "---  ------  --------------  -----\n",
      " 0   Temp1   7904 non-null   int64\n",
      " 1   Temp2   7904 non-null   int64\n",
      " 2   Temp3   7904 non-null   int64\n",
      " 3   Temp4   7904 non-null   int64\n",
      " 4   Temp5   7904 non-null   int64\n",
      " 5   Temp6   7904 non-null   int64\n",
      " 6   Temp7   7904 non-null   int64\n",
      " 7   Temp8   7904 non-null   int64\n",
      " 8   Temp9   7904 non-null   int64\n",
      " 9   Temp10  7904 non-null   int64\n",
      " 10  c1_off  7904 non-null   int64\n",
      " 11  c1_16h  7904 non-null   int64\n",
      " 12  c1_16l  7904 non-null   int64\n",
      " 13  c1_23h  7904 non-null   int64\n",
      " 14  c1_23l  7904 non-null   int64\n",
      " 15  c1_32h  7904 non-null   int64\n",
      " 16  c1_32l  7904 non-null   int64\n",
      " 17  c2_off  7904 non-null   int64\n",
      " 18  c2_16h  7904 non-null   int64\n",
      " 19  c2_16l  7904 non-null   int64\n",
      " 20  c2_23h  7904 non-null   int64\n",
      " 21  c2_23l  7904 non-null   int64\n",
      " 22  c2_32h  7904 non-null   int64\n",
      " 23  c2_32l  7904 non-null   int64\n",
      "dtypes: int64(24)\n",
      "memory usage: 1.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cell_id": "93523c5602b44dc5a0e64b09e1d293c4",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 245,
    "execution_start": 1672111434317,
    "source_hash": "f88152d9",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Temp1</th>\n",
       "      <th>Temp2</th>\n",
       "      <th>Temp3</th>\n",
       "      <th>Temp4</th>\n",
       "      <th>Temp5</th>\n",
       "      <th>Temp6</th>\n",
       "      <th>Temp7</th>\n",
       "      <th>Temp8</th>\n",
       "      <th>Temp9</th>\n",
       "      <th>Temp10</th>\n",
       "      <th>...</th>\n",
       "      <th>c1_23l</th>\n",
       "      <th>c1_32h</th>\n",
       "      <th>c1_32l</th>\n",
       "      <th>c2_off</th>\n",
       "      <th>c2_16h</th>\n",
       "      <th>c2_16l</th>\n",
       "      <th>c2_23h</th>\n",
       "      <th>c2_23l</th>\n",
       "      <th>c2_32h</th>\n",
       "      <th>c2_32l</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7904.000000</td>\n",
       "      <td>7904.000000</td>\n",
       "      <td>7904.000000</td>\n",
       "      <td>7904.000000</td>\n",
       "      <td>7904.000000</td>\n",
       "      <td>7904.000000</td>\n",
       "      <td>7904.000000</td>\n",
       "      <td>7904.000000</td>\n",
       "      <td>7904.000000</td>\n",
       "      <td>7904.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>7904.000000</td>\n",
       "      <td>7904.000000</td>\n",
       "      <td>7904.000000</td>\n",
       "      <td>7904.000000</td>\n",
       "      <td>7904.000000</td>\n",
       "      <td>7904.000000</td>\n",
       "      <td>7904.000000</td>\n",
       "      <td>7904.000000</td>\n",
       "      <td>7904.000000</td>\n",
       "      <td>7904.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>23.434464</td>\n",
       "      <td>22.831225</td>\n",
       "      <td>23.092105</td>\n",
       "      <td>23.293016</td>\n",
       "      <td>23.669787</td>\n",
       "      <td>22.863360</td>\n",
       "      <td>22.904099</td>\n",
       "      <td>23.871711</td>\n",
       "      <td>23.417004</td>\n",
       "      <td>23.451164</td>\n",
       "      <td>...</td>\n",
       "      <td>0.150810</td>\n",
       "      <td>0.078188</td>\n",
       "      <td>0.144484</td>\n",
       "      <td>0.233806</td>\n",
       "      <td>0.040233</td>\n",
       "      <td>0.127530</td>\n",
       "      <td>0.224949</td>\n",
       "      <td>0.150810</td>\n",
       "      <td>0.078188</td>\n",
       "      <td>0.144484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.998546</td>\n",
       "      <td>2.008613</td>\n",
       "      <td>2.017667</td>\n",
       "      <td>2.001435</td>\n",
       "      <td>2.050981</td>\n",
       "      <td>2.019406</td>\n",
       "      <td>2.034540</td>\n",
       "      <td>2.012862</td>\n",
       "      <td>2.049790</td>\n",
       "      <td>2.027807</td>\n",
       "      <td>...</td>\n",
       "      <td>0.357886</td>\n",
       "      <td>0.268485</td>\n",
       "      <td>0.351602</td>\n",
       "      <td>0.423277</td>\n",
       "      <td>0.196517</td>\n",
       "      <td>0.333587</td>\n",
       "      <td>0.417575</td>\n",
       "      <td>0.357886</td>\n",
       "      <td>0.268485</td>\n",
       "      <td>0.351602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>17.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>22.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>24.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>25.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>30.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Temp1        Temp2        Temp3        Temp4        Temp5  \\\n",
       "count  7904.000000  7904.000000  7904.000000  7904.000000  7904.000000   \n",
       "mean     23.434464    22.831225    23.092105    23.293016    23.669787   \n",
       "std       1.998546     2.008613     2.017667     2.001435     2.050981   \n",
       "min      17.000000    16.000000    16.000000    17.000000    16.000000   \n",
       "25%      22.000000    22.000000    22.000000    22.000000    22.000000   \n",
       "50%      24.000000    23.000000    23.000000    23.000000    24.000000   \n",
       "75%      25.000000    24.000000    24.000000    25.000000    25.000000   \n",
       "max      30.000000    30.000000    29.000000    29.000000    30.000000   \n",
       "\n",
       "             Temp6        Temp7        Temp8        Temp9       Temp10  ...  \\\n",
       "count  7904.000000  7904.000000  7904.000000  7904.000000  7904.000000  ...   \n",
       "mean     22.863360    22.904099    23.871711    23.417004    23.451164  ...   \n",
       "std       2.019406     2.034540     2.012862     2.049790     2.027807  ...   \n",
       "min      16.000000    16.000000    17.000000    17.000000    17.000000  ...   \n",
       "25%      22.000000    22.000000    23.000000    22.000000    22.000000  ...   \n",
       "50%      23.000000    23.000000    24.000000    24.000000    24.000000  ...   \n",
       "75%      24.000000    24.000000    25.000000    25.000000    25.000000  ...   \n",
       "max      29.000000    29.000000    30.000000    35.000000    36.000000  ...   \n",
       "\n",
       "            c1_23l       c1_32h       c1_32l       c2_off       c2_16h  \\\n",
       "count  7904.000000  7904.000000  7904.000000  7904.000000  7904.000000   \n",
       "mean      0.150810     0.078188     0.144484     0.233806     0.040233   \n",
       "std       0.357886     0.268485     0.351602     0.423277     0.196517   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "50%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "75%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "max       1.000000     1.000000     1.000000     1.000000     1.000000   \n",
       "\n",
       "            c2_16l       c2_23h       c2_23l       c2_32h       c2_32l  \n",
       "count  7904.000000  7904.000000  7904.000000  7904.000000  7904.000000  \n",
       "mean      0.127530     0.224949     0.150810     0.078188     0.144484  \n",
       "std       0.333587     0.417575     0.357886     0.268485     0.351602  \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000  \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000  \n",
       "50%       0.000000     0.000000     0.000000     0.000000     0.000000  \n",
       "75%       0.000000     0.000000     0.000000     0.000000     0.000000  \n",
       "max       1.000000     1.000000     1.000000     1.000000     1.000000  \n",
       "\n",
       "[8 rows x 24 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cell_id": "088815d8f93446da99b06b0fdee77263",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 4,
    "execution_start": 1672111434560,
    "source_hash": "14f60b8f",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7904, 24)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cell_id": "2e4b2edbc0954d08b586b8aa908cbd92",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 5,
    "execution_start": 1672111434564,
    "source_hash": "ccd0ad9b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "# set options temporarily to display one large DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "cell_id": "661b75e8fd4f428e9da3f2a4260ab5f0",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 1,
    "execution_start": 1672119707723,
    "source_hash": "ef59bbdf"
   },
   "outputs": [],
   "source": [
    "#x = df.iloc[:-1, -14:]\n",
    "#y = df.iloc[1:, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "cell_id": "ee9f27068cb64e96bf76eb55d2ab35e4",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 61,
    "execution_start": 1672120186329,
    "source_hash": "e0a75a46",
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = df.iloc[:-1, -14:]\n",
    "x [x != 1] =  (0.1 , 0.2 , 0.3 , 0.4, 0.5, 0.6, 0.7, 0.8, 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "cell_id": "56f8d08a79be476abf791651ee87bcef",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 71,
    "execution_start": 1672120253736,
    "source_hash": "6b24cdd5",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>c1_off</th>\n",
       "      <th>c1_16h</th>\n",
       "      <th>c1_16l</th>\n",
       "      <th>c1_23h</th>\n",
       "      <th>c1_23l</th>\n",
       "      <th>c1_32h</th>\n",
       "      <th>c1_32l</th>\n",
       "      <th>c2_off</th>\n",
       "      <th>c2_16h</th>\n",
       "      <th>c2_16l</th>\n",
       "      <th>c2_23h</th>\n",
       "      <th>c2_23l</th>\n",
       "      <th>c2_32h</th>\n",
       "      <th>c2_32l</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-02-16 13:44:23</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-16 13:54:23</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-16 14:04:23</th>\n",
       "      <td>0.3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-16 14:14:23</th>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-16 14:24:23</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-04-12 10:04:46</th>\n",
       "      <td>0.6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-04-12 10:14:46</th>\n",
       "      <td>0.7</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-04-12 10:24:46</th>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-04-12 10:34:46</th>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-04-12 10:44:46</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7903 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     c1_off  c1_16h  c1_16l  c1_23h  c1_23l  c1_32h  c1_32l  \\\n",
       "Time                                                                          \n",
       "2021-02-16 13:44:23     0.1     0.1     0.1     0.1     1.0     0.1     0.1   \n",
       "2021-02-16 13:54:23     0.2     0.2     0.2     0.2     1.0     0.2     0.2   \n",
       "2021-02-16 14:04:23     0.3     0.3     0.3     0.3     1.0     0.3     0.3   \n",
       "2021-02-16 14:14:23     0.4     0.4     0.4     0.4     1.0     0.4     0.4   \n",
       "2021-02-16 14:24:23     0.5     0.5     0.5     0.5     1.0     0.5     0.5   \n",
       "...                     ...     ...     ...     ...     ...     ...     ...   \n",
       "2021-04-12 10:04:46     0.6     0.6     0.6     0.6     0.6     1.0     0.6   \n",
       "2021-04-12 10:14:46     0.7     0.7     0.7     0.7     0.7     1.0     0.7   \n",
       "2021-04-12 10:24:46     0.8     0.8     0.8     0.8     0.8     1.0     0.8   \n",
       "2021-04-12 10:34:46     0.9     0.9     0.9     0.9     0.9     1.0     0.9   \n",
       "2021-04-12 10:44:46     0.1     0.1     0.1     0.1     0.1     1.0     0.1   \n",
       "\n",
       "                     c2_off  c2_16h  c2_16l  c2_23h  c2_23l  c2_32h  c2_32l  \n",
       "Time                                                                         \n",
       "2021-02-16 13:44:23     0.1     0.1     0.1     0.1     1.0     0.1     0.1  \n",
       "2021-02-16 13:54:23     0.2     0.2     0.2     0.2     1.0     0.2     0.2  \n",
       "2021-02-16 14:04:23     0.3     0.3     0.3     0.3     1.0     0.3     0.3  \n",
       "2021-02-16 14:14:23     0.4     0.4     0.4     0.4     1.0     0.4     0.4  \n",
       "2021-02-16 14:24:23     0.5     0.5     0.5     0.5     1.0     0.5     0.5  \n",
       "...                     ...     ...     ...     ...     ...     ...     ...  \n",
       "2021-04-12 10:04:46     0.6     0.6     0.6     0.6     0.6     1.0     0.6  \n",
       "2021-04-12 10:14:46     0.7     0.7     0.7     0.7     0.7     1.0     0.7  \n",
       "2021-04-12 10:24:46     0.8     0.8     0.8     0.8     0.8     1.0     0.8  \n",
       "2021-04-12 10:34:46     0.9     0.9     0.9     0.9     0.9     1.0     0.9  \n",
       "2021-04-12 10:44:46     0.1     0.1     0.1     0.1     0.1     1.0     0.1  \n",
       "\n",
       "[7903 rows x 14 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "cell_id": "5512e84262584025aa0a4bb9aca8939d",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 47,
    "execution_start": 1672120230231,
    "source_hash": "261ce3b1",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Temp1</th>\n",
       "      <th>Temp2</th>\n",
       "      <th>Temp3</th>\n",
       "      <th>Temp4</th>\n",
       "      <th>Temp5</th>\n",
       "      <th>Temp6</th>\n",
       "      <th>Temp7</th>\n",
       "      <th>Temp8</th>\n",
       "      <th>Temp9</th>\n",
       "      <th>Temp10</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-02-16 13:54:23</th>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>23</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-16 14:04:23</th>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>23</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>23</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-16 14:14:23</th>\n",
       "      <td>23</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-16 14:24:23</th>\n",
       "      <td>23</td>\n",
       "      <td>22</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-16 14:34:23</th>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>24</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>24</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-04-12 10:14:46</th>\n",
       "      <td>19</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>19</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>19</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-04-12 10:24:46</th>\n",
       "      <td>19</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>19</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>19</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-04-12 10:34:46</th>\n",
       "      <td>19</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>19</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>19</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-04-12 10:44:46</th>\n",
       "      <td>19</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>19</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>19</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-04-12 10:54:46</th>\n",
       "      <td>19</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>19</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>19</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7903 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Temp1  Temp2  Temp3  Temp4  Temp5  Temp6  Temp7  Temp8  \\\n",
       "Time                                                                          \n",
       "2021-02-16 13:54:23     21     21     22     22     22     22     22     23   \n",
       "2021-02-16 14:04:23     22     22     22     22     23     22     22     23   \n",
       "2021-02-16 14:14:23     23     22     22     23     23     22     22     23   \n",
       "2021-02-16 14:24:23     23     22     23     23     23     23     23     23   \n",
       "2021-02-16 14:34:23     23     23     23     23     24     23     23     24   \n",
       "...                    ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "2021-04-12 10:14:46     19     18     18     18     19     18     18     19   \n",
       "2021-04-12 10:24:46     19     18     18     18     19     18     18     19   \n",
       "2021-04-12 10:34:46     19     18     18     18     19     18     18     19   \n",
       "2021-04-12 10:44:46     19     18     18     18     19     18     18     19   \n",
       "2021-04-12 10:54:46     19     18     18     18     19     18     18     19   \n",
       "\n",
       "                     Temp9  Temp10  \n",
       "Time                                \n",
       "2021-02-16 13:54:23     22      22  \n",
       "2021-02-16 14:04:23     22      22  \n",
       "2021-02-16 14:14:23     23      23  \n",
       "2021-02-16 14:24:23     23      23  \n",
       "2021-02-16 14:34:23     23      23  \n",
       "...                    ...     ...  \n",
       "2021-04-12 10:14:46     18      18  \n",
       "2021-04-12 10:24:46     18      18  \n",
       "2021-04-12 10:34:46     18      18  \n",
       "2021-04-12 10:44:46     18      18  \n",
       "2021-04-12 10:54:46     18      18  \n",
       "\n",
       "[7903 rows x 10 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = df.iloc[1:, :10]\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "cell_id": "9e84e5588c604a7a964b3f310fe55c73",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 5,
    "execution_start": 1672120272885,
    "source_hash": "57b8ea20",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7903, 10)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "cell_id": "9d1d9c6bef6240f6bcb3d243e2889da6",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 166359,
    "execution_start": 1672111434630,
    "scrolled": false,
    "source_hash": "894a8052"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 100)               1500      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 20)                2020      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 20)                420       \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 20)                420       \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 10)                210       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,570\n",
      "Trainable params: 4,570\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "173/173 - 2s - loss: 506.7909 - accuracy: 0.2787 - val_loss: 495.8476 - val_accuracy: 0.2206 - 2s/epoch - 12ms/step\n",
      "Epoch 2/50\n",
      "173/173 - 0s - loss: 502.8427 - accuracy: 0.2164 - val_loss: 495.8347 - val_accuracy: 0.2598 - 250ms/epoch - 1ms/step\n",
      "Epoch 3/50\n",
      "173/173 - 0s - loss: 502.8366 - accuracy: 0.2453 - val_loss: 495.8326 - val_accuracy: 0.2809 - 251ms/epoch - 1ms/step\n",
      "Epoch 4/50\n",
      "173/173 - 0s - loss: 502.8352 - accuracy: 0.2652 - val_loss: 495.8318 - val_accuracy: 0.3125 - 235ms/epoch - 1ms/step\n",
      "Epoch 5/50\n",
      "173/173 - 0s - loss: 502.8348 - accuracy: 0.2907 - val_loss: 495.8316 - val_accuracy: 0.3357 - 246ms/epoch - 1ms/step\n",
      "Epoch 6/50\n",
      "173/173 - 0s - loss: 502.8343 - accuracy: 0.3221 - val_loss: 495.8313 - val_accuracy: 0.3593 - 236ms/epoch - 1ms/step\n",
      "Epoch 7/50\n",
      "173/173 - 0s - loss: 502.8340 - accuracy: 0.3348 - val_loss: 495.8312 - val_accuracy: 0.3741 - 248ms/epoch - 1ms/step\n",
      "Epoch 8/50\n",
      "173/173 - 0s - loss: 502.8342 - accuracy: 0.3485 - val_loss: 495.8310 - val_accuracy: 0.3846 - 261ms/epoch - 2ms/step\n",
      "Epoch 9/50\n",
      "173/173 - 0s - loss: 502.8341 - accuracy: 0.3487 - val_loss: 495.8310 - val_accuracy: 0.3851 - 263ms/epoch - 2ms/step\n",
      "Epoch 10/50\n",
      "173/173 - 0s - loss: 502.8339 - accuracy: 0.3498 - val_loss: 495.8309 - val_accuracy: 0.3851 - 252ms/epoch - 1ms/step\n",
      "Epoch 11/50\n",
      "173/173 - 0s - loss: 502.8339 - accuracy: 0.3500 - val_loss: 495.8309 - val_accuracy: 0.3851 - 246ms/epoch - 1ms/step\n",
      "Epoch 12/50\n",
      "173/173 - 0s - loss: 502.8340 - accuracy: 0.3500 - val_loss: 495.8309 - val_accuracy: 0.3851 - 245ms/epoch - 1ms/step\n",
      "Epoch 13/50\n",
      "173/173 - 0s - loss: 502.8341 - accuracy: 0.3568 - val_loss: 495.8309 - val_accuracy: 0.3851 - 248ms/epoch - 1ms/step\n",
      "Epoch 14/50\n",
      "173/173 - 0s - loss: 502.8341 - accuracy: 0.3619 - val_loss: 495.8309 - val_accuracy: 0.4007 - 267ms/epoch - 2ms/step\n",
      "Epoch 15/50\n",
      "173/173 - 0s - loss: 502.8339 - accuracy: 0.3642 - val_loss: 495.8309 - val_accuracy: 0.3855 - 249ms/epoch - 1ms/step\n",
      "Epoch 16/50\n",
      "173/173 - 0s - loss: 502.8341 - accuracy: 0.3686 - val_loss: 495.8309 - val_accuracy: 0.4121 - 250ms/epoch - 1ms/step\n",
      "Epoch 17/50\n",
      "173/173 - 0s - loss: 502.8340 - accuracy: 0.3709 - val_loss: 495.8309 - val_accuracy: 0.4188 - 305ms/epoch - 2ms/step\n",
      "Epoch 18/50\n",
      "173/173 - 0s - loss: 502.8340 - accuracy: 0.3778 - val_loss: 495.8309 - val_accuracy: 0.4133 - 254ms/epoch - 1ms/step\n",
      "Epoch 19/50\n",
      "173/173 - 0s - loss: 502.8340 - accuracy: 0.3776 - val_loss: 495.8309 - val_accuracy: 0.4407 - 256ms/epoch - 1ms/step\n",
      "Epoch 20/50\n",
      "173/173 - 0s - loss: 502.8339 - accuracy: 0.3839 - val_loss: 495.8309 - val_accuracy: 0.4475 - 265ms/epoch - 2ms/step\n",
      "Epoch 21/50\n",
      "173/173 - 0s - loss: 502.8340 - accuracy: 0.3890 - val_loss: 495.8309 - val_accuracy: 0.4319 - 300ms/epoch - 2ms/step\n",
      "Epoch 22/50\n",
      "173/173 - 0s - loss: 502.8340 - accuracy: 0.3899 - val_loss: 495.8309 - val_accuracy: 0.4479 - 252ms/epoch - 1ms/step\n",
      "Epoch 23/50\n",
      "173/173 - 0s - loss: 502.8340 - accuracy: 0.3971 - val_loss: 495.8309 - val_accuracy: 0.4424 - 262ms/epoch - 2ms/step\n",
      "Epoch 24/50\n",
      "173/173 - 0s - loss: 502.8340 - accuracy: 0.4031 - val_loss: 495.8309 - val_accuracy: 0.4635 - 265ms/epoch - 2ms/step\n",
      "Epoch 25/50\n",
      "173/173 - 0s - loss: 502.8339 - accuracy: 0.4087 - val_loss: 495.8309 - val_accuracy: 0.4555 - 243ms/epoch - 1ms/step\n",
      "Epoch 26/50\n",
      "173/173 - 0s - loss: 502.8341 - accuracy: 0.4172 - val_loss: 495.8309 - val_accuracy: 0.4479 - 263ms/epoch - 2ms/step\n",
      "Epoch 27/50\n",
      "173/173 - 0s - loss: 502.8340 - accuracy: 0.4225 - val_loss: 495.8309 - val_accuracy: 0.4880 - 272ms/epoch - 2ms/step\n",
      "Epoch 28/50\n",
      "173/173 - 0s - loss: 502.8340 - accuracy: 0.4369 - val_loss: 495.8309 - val_accuracy: 0.4871 - 248ms/epoch - 1ms/step\n",
      "Epoch 29/50\n",
      "173/173 - 0s - loss: 502.8339 - accuracy: 0.4376 - val_loss: 495.8309 - val_accuracy: 0.4876 - 296ms/epoch - 2ms/step\n",
      "Epoch 30/50\n",
      "173/173 - 0s - loss: 502.8339 - accuracy: 0.4492 - val_loss: 495.8309 - val_accuracy: 0.5023 - 262ms/epoch - 2ms/step\n",
      "Epoch 31/50\n",
      "173/173 - 0s - loss: 502.8340 - accuracy: 0.4508 - val_loss: 495.8309 - val_accuracy: 0.4905 - 259ms/epoch - 1ms/step\n",
      "Epoch 32/50\n",
      "173/173 - 0s - loss: 502.8340 - accuracy: 0.4537 - val_loss: 495.8309 - val_accuracy: 0.5053 - 280ms/epoch - 2ms/step\n",
      "Epoch 33/50\n",
      "173/173 - 0s - loss: 502.8340 - accuracy: 0.4640 - val_loss: 495.8309 - val_accuracy: 0.5386 - 269ms/epoch - 2ms/step\n",
      "Epoch 34/50\n",
      "173/173 - 0s - loss: 502.8339 - accuracy: 0.4732 - val_loss: 495.8309 - val_accuracy: 0.5268 - 247ms/epoch - 1ms/step\n",
      "Epoch 35/50\n",
      "173/173 - 0s - loss: 502.8341 - accuracy: 0.4720 - val_loss: 495.8309 - val_accuracy: 0.5217 - 264ms/epoch - 2ms/step\n",
      "Epoch 36/50\n",
      "173/173 - 0s - loss: 502.8340 - accuracy: 0.4758 - val_loss: 495.8309 - val_accuracy: 0.5272 - 284ms/epoch - 2ms/step\n",
      "Epoch 37/50\n",
      "173/173 - 0s - loss: 502.8339 - accuracy: 0.4805 - val_loss: 495.8309 - val_accuracy: 0.5390 - 283ms/epoch - 2ms/step\n",
      "Epoch 38/50\n",
      "173/173 - 0s - loss: 502.8339 - accuracy: 0.4816 - val_loss: 495.8309 - val_accuracy: 0.5285 - 266ms/epoch - 2ms/step\n",
      "Epoch 39/50\n",
      "173/173 - 0s - loss: 502.8340 - accuracy: 0.4836 - val_loss: 495.8309 - val_accuracy: 0.5335 - 252ms/epoch - 1ms/step\n",
      "Epoch 40/50\n",
      "173/173 - 0s - loss: 502.8339 - accuracy: 0.4787 - val_loss: 495.8309 - val_accuracy: 0.5335 - 253ms/epoch - 1ms/step\n",
      "Epoch 41/50\n",
      "173/173 - 0s - loss: 502.8340 - accuracy: 0.4769 - val_loss: 495.8309 - val_accuracy: 0.5272 - 256ms/epoch - 1ms/step\n",
      "Epoch 42/50\n",
      "173/173 - 0s - loss: 502.8339 - accuracy: 0.4828 - val_loss: 495.8309 - val_accuracy: 0.5340 - 254ms/epoch - 1ms/step\n",
      "Epoch 43/50\n",
      "173/173 - 0s - loss: 502.8339 - accuracy: 0.4917 - val_loss: 495.8309 - val_accuracy: 0.5525 - 236ms/epoch - 1ms/step\n",
      "Epoch 44/50\n",
      "173/173 - 0s - loss: 502.8341 - accuracy: 0.4953 - val_loss: 495.8309 - val_accuracy: 0.5487 - 267ms/epoch - 2ms/step\n",
      "Epoch 45/50\n",
      "173/173 - 0s - loss: 502.8337 - accuracy: 0.4904 - val_loss: 495.8309 - val_accuracy: 0.5340 - 247ms/epoch - 1ms/step\n",
      "Epoch 46/50\n",
      "173/173 - 0s - loss: 502.8338 - accuracy: 0.4899 - val_loss: 495.8309 - val_accuracy: 0.5677 - 252ms/epoch - 1ms/step\n",
      "Epoch 47/50\n",
      "173/173 - 0s - loss: 502.8339 - accuracy: 0.4960 - val_loss: 495.8309 - val_accuracy: 0.5525 - 256ms/epoch - 1ms/step\n",
      "Epoch 48/50\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_1884\\4149791169.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'mean_squared_error'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'adam'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m     \u001b[0mfit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_transf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1692\u001b[0m                             \u001b[0msteps_per_execution\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_steps_per_execution\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1693\u001b[0m                         )\n\u001b[1;32m-> 1694\u001b[1;33m                     val_logs = self.evaluate(\n\u001b[0m\u001b[0;32m   1695\u001b[0m                         \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_x\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1696\u001b[0m                         \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_y\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[0;32m   2038\u001b[0m                         ):\n\u001b[0;32m   2039\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_test_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2040\u001b[1;33m                             \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2041\u001b[0m                             \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2042\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    878\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 880\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    881\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    882\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    917\u001b[0m       \u001b[1;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    918\u001b[0m       \u001b[1;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 919\u001b[1;33m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_variable_creation_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    920\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_variables\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mALLOW_DYNAMIC_VARIABLE_CREATION\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    921\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    132\u001b[0m       (concrete_function,\n\u001b[0;32m    133\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m--> 134\u001b[1;33m     return concrete_function._call_flat(\n\u001b[0m\u001b[0;32m    135\u001b[0m         filtered_flat_args, captured_inputs=concrete_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m    136\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1743\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1744\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1745\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1746\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    376\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    377\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 378\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    379\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    380\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     50\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     53\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     54\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# It Allows You to Execute Code When the File Runs as a Script\n",
    "if __name__ == '__main__':\n",
    "    scaler_x = StandardScaler()\n",
    "    scaler_x.fit(x)\n",
    "    X_transf = scaler_x.transform(x)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_transf, y, shuffle=False)\n",
    "    # Split the dataset into training and testing sets\n",
    "    model = Sequential()\n",
    "    model.add(Dense(100, input_dim=14, kernel_initializer='he_uniform', activation='relu'))\n",
    "    model.add(Dense(20, activation='relu'))\n",
    "    model.add(Dense(20, activation='relu'))\n",
    "    model.add(Dense(20, activation='relu'))\n",
    "    model.add(Dense(10, activation='sigmoid'))\n",
    "    model.summary()\n",
    "\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
    "    fit = model.fit(X_transf, y, epochs=50, verbose=2, validation_split=0.3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "cell_id": "f86a182825d24a43bc32925f28c0e1fb",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 11,
    "execution_start": 1672123612048,
    "source_hash": "4c873233",
    "tags": []
   },
   "outputs": [],
   "source": [
    "scaler1 = MinMaxScaler()\n",
    "X_train_scaled1 = scaler1.fit_transform(X_train)\n",
    "X_test_scaled1 = scaler1.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "cell_id": "0464b96da6524ab9a0e8092c4762fd21",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 113,
    "execution_start": 1672111601038,
    "source_hash": "a5c3029d",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error: 1.70\n"
     ]
    }
   ],
   "source": [
    "#  linear regression model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "model1 = LinearRegression()\n",
    "model1.fit(X_train_scaled1, y_train)\n",
    "\n",
    "# Test the model on the preprocessed testing data\n",
    "y_pred = model1.predict(X_test_scaled1)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean squared error: {mse:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "cell_id": "ff4b6acafd234cb68f1cd3d2580a6ba7",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 0,
    "execution_start": 1672123502631,
    "source_hash": "ece07c98",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "cell_id": "e4f73fc4d18f4ea98d16eabc30771f58",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 372573,
    "execution_start": 1672124884624,
    "source_hash": "65799d57",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/80\n",
      "186/186 [==============================] - 1s 1ms/step - loss: 548.4076\n",
      "Epoch 2/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 538.8697\n",
      "Epoch 3/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 530.3645\n",
      "Epoch 4/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 521.9538\n",
      "Epoch 5/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 513.6371\n",
      "Epoch 6/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 505.4134\n",
      "Epoch 7/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 497.2780\n",
      "Epoch 8/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 489.2298\n",
      "Epoch 9/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 481.2692\n",
      "Epoch 10/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 473.3922\n",
      "Epoch 11/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 465.5960\n",
      "Epoch 12/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 457.8834\n",
      "Epoch 13/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 450.2469\n",
      "Epoch 14/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 442.6864\n",
      "Epoch 15/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 435.2054\n",
      "Epoch 16/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 427.7964\n",
      "Epoch 17/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 420.4635\n",
      "Epoch 18/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 413.2027\n",
      "Epoch 19/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 406.0130\n",
      "Epoch 20/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 398.8950\n",
      "Epoch 21/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 391.8470\n",
      "Epoch 22/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 384.8700\n",
      "Epoch 23/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 377.9624\n",
      "Epoch 24/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 371.1235\n",
      "Epoch 25/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 364.3532\n",
      "Epoch 26/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 357.6525\n",
      "Epoch 27/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 351.0172\n",
      "Epoch 28/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 344.4519\n",
      "Epoch 29/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 337.9518\n",
      "Epoch 30/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 331.5177\n",
      "Epoch 31/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 325.1511\n",
      "Epoch 32/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 318.8516\n",
      "Epoch 33/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 312.6181\n",
      "Epoch 34/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 306.4519\n",
      "Epoch 35/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 300.3488\n",
      "Epoch 36/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 294.3120\n",
      "Epoch 37/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 288.3423\n",
      "Epoch 38/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 282.4382\n",
      "Epoch 39/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 276.5984\n",
      "Epoch 40/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 270.8242\n",
      "Epoch 41/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 265.1161\n",
      "Epoch 42/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 259.4754\n",
      "Epoch 43/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 253.9001\n",
      "Epoch 44/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 248.3882\n",
      "Epoch 45/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 242.9409\n",
      "Epoch 46/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 237.5577\n",
      "Epoch 47/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 232.2409\n",
      "Epoch 48/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 226.9884\n",
      "Epoch 49/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 221.7995\n",
      "Epoch 50/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 216.6761\n",
      "Epoch 51/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 211.6173\n",
      "Epoch 52/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 206.6219\n",
      "Epoch 53/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 201.6933\n",
      "Epoch 54/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 196.8286\n",
      "Epoch 55/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 192.0267\n",
      "Epoch 56/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 187.2890\n",
      "Epoch 57/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 182.6152\n",
      "Epoch 58/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 178.0062\n",
      "Epoch 59/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 173.4620\n",
      "Epoch 60/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 168.9819\n",
      "Epoch 61/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 164.5651\n",
      "Epoch 62/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 160.2138\n",
      "Epoch 63/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 155.9253\n",
      "Epoch 64/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 151.6998\n",
      "Epoch 65/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 147.5389\n",
      "Epoch 66/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 143.4423\n",
      "Epoch 67/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 139.4080\n",
      "Epoch 68/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 135.4361\n",
      "Epoch 69/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 131.5279\n",
      "Epoch 70/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 127.6844\n",
      "Epoch 71/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 123.9046\n",
      "Epoch 72/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 120.1872\n",
      "Epoch 73/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 116.5342\n",
      "Epoch 74/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 112.9411\n",
      "Epoch 75/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 109.4133\n",
      "Epoch 76/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 105.9462\n",
      "Epoch 77/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 102.5429\n",
      "Epoch 78/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 99.2016\n",
      "Epoch 79/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 95.9215\n",
      "Epoch 80/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 92.7038\n",
      "62/62 [==============================] - 0s 841us/step - loss: 83.3326\n",
      "Mean squared error for activation relu: 83.33\n",
      "Epoch 1/80\n",
      "186/186 [==============================] - 1s 2ms/step - loss: 528.9991\n",
      "Epoch 2/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 506.3033\n",
      "Epoch 3/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 483.8659\n",
      "Epoch 4/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 461.1700\n",
      "Epoch 5/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 437.6505\n",
      "Epoch 6/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 415.2610\n",
      "Epoch 7/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 395.7860\n",
      "Epoch 8/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 378.5759\n",
      "Epoch 9/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 362.8122\n",
      "Epoch 10/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 348.0596\n",
      "Epoch 11/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 334.0693\n",
      "Epoch 12/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 320.6924\n",
      "Epoch 13/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 307.8354\n",
      "Epoch 14/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 295.4316\n",
      "Epoch 15/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 283.4474\n",
      "Epoch 16/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 271.8373\n",
      "Epoch 17/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 260.5798\n",
      "Epoch 18/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 249.6547\n",
      "Epoch 19/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 239.0425\n",
      "Epoch 20/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 228.7296\n",
      "Epoch 21/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 218.7119\n",
      "Epoch 22/80\n",
      "186/186 [==============================] - 0s 3ms/step - loss: 208.9774\n",
      "Epoch 23/80\n",
      "186/186 [==============================] - 1s 3ms/step - loss: 199.5162\n",
      "Epoch 24/80\n",
      "186/186 [==============================] - 1s 3ms/step - loss: 190.3211\n",
      "Epoch 25/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 181.3891\n",
      "Epoch 26/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 172.7178\n",
      "Epoch 27/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 164.2991\n",
      "Epoch 28/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 156.1313\n",
      "Epoch 29/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 148.2125\n",
      "Epoch 30/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 140.5376\n",
      "Epoch 31/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 133.1046\n",
      "Epoch 32/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 125.9116\n",
      "Epoch 33/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 118.9586\n",
      "Epoch 34/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 112.2379\n",
      "Epoch 35/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 105.7511\n",
      "Epoch 36/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 99.4900\n",
      "Epoch 37/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 93.4608\n",
      "Epoch 38/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 87.6606\n",
      "Epoch 39/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 82.0831\n",
      "Epoch 40/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 76.7277\n",
      "Epoch 41/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 71.5944\n",
      "Epoch 42/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 66.6775\n",
      "Epoch 43/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 61.9792\n",
      "Epoch 44/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 57.4927\n",
      "Epoch 45/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 53.2202\n",
      "Epoch 46/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 49.1568\n",
      "Epoch 47/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 45.2976\n",
      "Epoch 48/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 41.6436\n",
      "Epoch 49/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 38.1920\n",
      "Epoch 50/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 34.9408\n",
      "Epoch 51/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 31.8828\n",
      "Epoch 52/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 29.0173\n",
      "Epoch 53/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 26.3422\n",
      "Epoch 54/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 23.8522\n",
      "Epoch 55/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 21.5373\n",
      "Epoch 56/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 19.4004\n",
      "Epoch 57/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 17.4349\n",
      "Epoch 58/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 15.6380\n",
      "Epoch 59/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 14.0008\n",
      "Epoch 60/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 12.5200\n",
      "Epoch 61/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 11.1899\n",
      "Epoch 62/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 10.0025\n",
      "Epoch 63/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 8.9518\n",
      "Epoch 64/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 8.0321\n",
      "Epoch 65/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 7.2317\n",
      "Epoch 66/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 6.5447\n",
      "Epoch 67/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 5.9622\n",
      "Epoch 68/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 5.4737\n",
      "Epoch 69/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 5.0730\n",
      "Epoch 70/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 4.7495\n",
      "Epoch 71/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 4.4931\n",
      "Epoch 72/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 4.2949\n",
      "Epoch 73/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 4.1464\n",
      "Epoch 74/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 4.0369\n",
      "Epoch 75/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 3.9592\n",
      "Epoch 76/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 3.9062\n",
      "Epoch 77/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 3.8718\n",
      "Epoch 78/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 3.8501\n",
      "Epoch 79/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 3.8372\n",
      "Epoch 80/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 3.8297\n",
      "62/62 [==============================] - 0s 867us/step - loss: 5.3375\n",
      "Mean squared error for activation softmax: 5.34\n",
      "Epoch 1/80\n",
      "186/186 [==============================] - 1s 2ms/step - loss: 412.3489\n",
      "Epoch 2/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 323.5249\n",
      "Epoch 3/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 263.1981\n",
      "Epoch 4/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 214.7278\n",
      "Epoch 5/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 174.1603\n",
      "Epoch 6/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 140.0422\n",
      "Epoch 7/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 111.4551\n",
      "Epoch 8/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 87.6619\n",
      "Epoch 9/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 68.0781\n",
      "Epoch 10/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 52.1650\n",
      "Epoch 11/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 39.4341\n",
      "Epoch 12/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 29.4458\n",
      "Epoch 13/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 21.7761\n",
      "Epoch 14/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 16.0221\n",
      "Epoch 15/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 11.8360\n",
      "Epoch 16/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 8.8912\n",
      "Epoch 17/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 6.8984\n",
      "Epoch 18/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 5.5981\n",
      "Epoch 19/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 4.7924\n",
      "Epoch 20/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 4.3243\n",
      "Epoch 21/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 4.0646\n",
      "Epoch 22/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 3.9318\n",
      "Epoch 23/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 3.8693\n",
      "Epoch 24/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 3.8409\n",
      "Epoch 25/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 3.8287\n",
      "Epoch 26/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 3.8240\n",
      "Epoch 27/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 3.8226\n",
      "Epoch 28/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 3.8226\n",
      "Epoch 29/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 3.8222\n",
      "Epoch 30/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 3.8224\n",
      "Epoch 31/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 3.8226\n",
      "Epoch 32/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 3.8221\n",
      "Epoch 33/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 3.8224\n",
      "Epoch 34/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "186/186 [==============================] - 0s 2ms/step - loss: 3.8222\n",
      "Epoch 35/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 3.8222\n",
      "Epoch 36/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 3.8222\n",
      "Epoch 37/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 3.8219\n",
      "Epoch 38/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 3.8217\n",
      "Epoch 39/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 3.8205\n",
      "Epoch 40/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 3.8130\n",
      "Epoch 41/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 3.3792\n",
      "Epoch 42/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 1.4391\n",
      "Epoch 43/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 1.0728\n",
      "Epoch 44/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.9922\n",
      "Epoch 45/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.8990\n",
      "Epoch 46/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.8466\n",
      "Epoch 47/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.8240\n",
      "Epoch 48/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.8191\n",
      "Epoch 49/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.8117\n",
      "Epoch 50/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.8057\n",
      "Epoch 51/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 0.7944\n",
      "Epoch 52/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 0.7846\n",
      "Epoch 53/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 0.7760\n",
      "Epoch 54/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7694\n",
      "Epoch 55/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7677\n",
      "Epoch 56/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7626\n",
      "Epoch 57/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 0.7607\n",
      "Epoch 58/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7592\n",
      "Epoch 59/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7603\n",
      "Epoch 60/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 0.7591\n",
      "Epoch 61/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 0.7580\n",
      "Epoch 62/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7586\n",
      "Epoch 63/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7562\n",
      "Epoch 64/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 0.7580\n",
      "Epoch 65/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7570\n",
      "Epoch 66/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7576\n",
      "Epoch 67/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 0.7595\n",
      "Epoch 68/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7574\n",
      "Epoch 69/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7607\n",
      "Epoch 70/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 0.7574\n",
      "Epoch 71/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7580\n",
      "Epoch 72/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7587\n",
      "Epoch 73/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7579\n",
      "Epoch 74/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7575\n",
      "Epoch 75/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7609\n",
      "Epoch 76/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7568\n",
      "Epoch 77/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7575\n",
      "Epoch 78/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7568\n",
      "Epoch 79/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 0.7586\n",
      "Epoch 80/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 0.7582\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.8436\n",
      "Mean squared error for activation sigmoid: 0.84\n",
      "Epoch 1/80\n",
      "186/186 [==============================] - 1s 2ms/step - loss: 182.8621\n",
      "Epoch 2/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 1.7526\n",
      "Epoch 3/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 0.7966\n",
      "Epoch 4/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 0.7576\n",
      "Epoch 5/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 0.7561\n",
      "Epoch 6/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 0.7573\n",
      "Epoch 7/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7579\n",
      "Epoch 8/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7580\n",
      "Epoch 9/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7568\n",
      "Epoch 10/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 0.7579\n",
      "Epoch 11/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 0.7584\n",
      "Epoch 12/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7620\n",
      "Epoch 13/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7604\n",
      "Epoch 14/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 0.7631\n",
      "Epoch 15/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 0.7611\n",
      "Epoch 16/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 0.7635\n",
      "Epoch 17/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 0.7647\n",
      "Epoch 18/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 0.7679\n",
      "Epoch 19/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7646\n",
      "Epoch 20/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7632\n",
      "Epoch 21/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7670\n",
      "Epoch 22/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7660\n",
      "Epoch 23/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 0.7635\n",
      "Epoch 24/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7679\n",
      "Epoch 25/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7683\n",
      "Epoch 26/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7733\n",
      "Epoch 27/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 0.7683\n",
      "Epoch 28/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 0.7689\n",
      "Epoch 29/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7728\n",
      "Epoch 30/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7705\n",
      "Epoch 31/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7710\n",
      "Epoch 32/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 0.7690\n",
      "Epoch 33/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 0.7706\n",
      "Epoch 34/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7668\n",
      "Epoch 35/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7745\n",
      "Epoch 36/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7756\n",
      "Epoch 37/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7722\n",
      "Epoch 38/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 0.7757\n",
      "Epoch 39/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 0.7755\n",
      "Epoch 40/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 0.7715\n",
      "Epoch 41/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 0.7720\n",
      "Epoch 42/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 0.7765\n",
      "Epoch 43/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7713\n",
      "Epoch 44/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7725\n",
      "Epoch 45/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 0.7698\n",
      "Epoch 46/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 0.7733\n",
      "Epoch 47/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 0.7717\n",
      "Epoch 48/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7713\n",
      "Epoch 49/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7688\n",
      "Epoch 50/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7705\n",
      "Epoch 51/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7697\n",
      "Epoch 52/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7781\n",
      "Epoch 53/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 0.7823\n",
      "Epoch 54/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7680\n",
      "Epoch 55/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7709\n",
      "Epoch 56/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 0.7723\n",
      "Epoch 57/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7785\n",
      "Epoch 58/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7646\n",
      "Epoch 59/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 0.7679\n",
      "Epoch 60/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7707\n",
      "Epoch 61/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 0.7711\n",
      "Epoch 62/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 0.7685\n",
      "Epoch 63/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7701\n",
      "Epoch 64/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7770\n",
      "Epoch 65/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7676\n",
      "Epoch 66/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7721\n",
      "Epoch 67/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7717\n",
      "Epoch 68/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7671\n",
      "Epoch 69/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 0.7737\n",
      "Epoch 70/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 0.7676\n",
      "Epoch 71/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7647\n",
      "Epoch 72/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 0.7687\n",
      "Epoch 73/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7724\n",
      "Epoch 74/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7703\n",
      "Epoch 75/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 0.7712\n",
      "Epoch 76/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7683\n",
      "Epoch 77/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 0.7716\n",
      "Epoch 78/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 0.7700\n",
      "Epoch 79/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 0.7702\n",
      "Epoch 80/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 0.7795\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.8067\n",
      "Mean squared error for activation selu: 0.81\n",
      "Epoch 1/80\n",
      "186/186 [==============================] - 1s 1ms/step - loss: 427.5558\n",
      "Epoch 2/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 325.4640\n",
      "Epoch 3/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 267.6052\n",
      "Epoch 4/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 219.5923\n",
      "Epoch 5/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 178.9848\n",
      "Epoch 6/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 144.5486\n",
      "Epoch 7/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 115.5086\n",
      "Epoch 8/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 91.2269\n",
      "Epoch 9/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 71.1395\n",
      "Epoch 10/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 54.7374\n",
      "Epoch 11/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 41.5640\n",
      "Epoch 12/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 31.1628\n",
      "Epoch 13/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 23.1238\n",
      "Epoch 14/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 17.0592\n",
      "Epoch 15/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 12.6048\n",
      "Epoch 16/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 9.4363\n",
      "Epoch 17/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 7.2673\n",
      "Epoch 18/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 5.8410\n",
      "Epoch 19/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 4.9457\n",
      "Epoch 20/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 4.4132\n",
      "Epoch 21/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 4.1140\n",
      "Epoch 22/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 3.9564\n",
      "Epoch 23/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 3.8800\n",
      "Epoch 24/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 3.8442\n",
      "Epoch 25/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 3.8297\n",
      "Epoch 26/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 3.8242\n",
      "Epoch 27/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 3.8227\n",
      "Epoch 28/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 3.8220\n",
      "Epoch 29/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 3.8218\n",
      "Epoch 30/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 3.8213\n",
      "Epoch 31/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 3.8197\n",
      "Epoch 32/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 3.0987\n",
      "Epoch 33/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 2.7551\n",
      "Epoch 34/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 1.9950\n",
      "Epoch 35/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 1.7000\n",
      "Epoch 36/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 1.2467\n",
      "Epoch 37/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 1.0811\n",
      "Epoch 38/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 1.0211\n",
      "Epoch 39/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.9355\n",
      "Epoch 40/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.8837\n",
      "Epoch 41/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.8549\n",
      "Epoch 42/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.8326\n",
      "Epoch 43/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.8237\n",
      "Epoch 44/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 0.8192\n",
      "Epoch 45/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.8176\n",
      "Epoch 46/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.8029\n",
      "Epoch 47/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7921\n",
      "Epoch 48/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7855\n",
      "Epoch 49/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7804\n",
      "Epoch 50/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7752\n",
      "Epoch 51/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7745\n",
      "Epoch 52/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7689\n",
      "Epoch 53/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 0.7651\n",
      "Epoch 54/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7624\n",
      "Epoch 55/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7637\n",
      "Epoch 56/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7622\n",
      "Epoch 57/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 0.7649\n",
      "Epoch 58/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 0.7634\n",
      "Epoch 59/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7638\n",
      "Epoch 60/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 0.7617\n",
      "Epoch 61/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7614\n",
      "Epoch 62/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7626\n",
      "Epoch 63/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7599\n",
      "Epoch 64/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7608\n",
      "Epoch 65/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7636\n",
      "Epoch 66/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7606\n",
      "Epoch 67/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7642\n",
      "Epoch 68/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 0.7616\n",
      "Epoch 69/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7633\n",
      "Epoch 70/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7618\n",
      "Epoch 71/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7601\n",
      "Epoch 72/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 0.7627\n",
      "Epoch 73/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7623\n",
      "Epoch 74/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7604\n",
      "Epoch 75/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 0.7619\n",
      "Epoch 76/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7609\n",
      "Epoch 77/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7617\n",
      "Epoch 78/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7620\n",
      "Epoch 79/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 0.7610\n",
      "Epoch 80/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 0.7618\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.8284\n",
      "Mean squared error for activation tanh: 0.83\n"
     ]
    }
   ],
   "source": [
    "def activations(x):\n",
    "    model6 = keras.Sequential([\n",
    "        keras.layers.Dense(32, input_shape=(14,), activation=x),\n",
    "        keras.layers.Dense(16, activation=x),\n",
    "        keras.layers.Dense(8, activation=x),\n",
    "        keras.layers.Dense(1)\n",
    "\n",
    "    ])\n",
    "    \n",
    "    model6.compile(optimizer='adam', loss='mse')\n",
    "    \n",
    "    model6.fit(X_train_scaled1, y_train, epochs=80)\n",
    "    \n",
    "    loss = model6.evaluate(X_test_scaled1, y_test)\n",
    "    print(f'Mean squared error for activation {x}: {loss:.2f}')\n",
    "\n",
    "for i in ['relu', 'softmax', 'sigmoid', 'selu', 'tanh']:\n",
    "    activations(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/80\n",
      "186/186 [==============================] - 1s 2ms/step - loss: 244.8867\n",
      "Epoch 2/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.8462\n",
      "Epoch 3/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7566\n",
      "Epoch 4/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 0.7583\n",
      "Epoch 5/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7600\n",
      "Epoch 6/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7588\n",
      "Epoch 7/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7641\n",
      "Epoch 8/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 0.7599\n",
      "Epoch 9/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7644\n",
      "Epoch 10/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 0.7652\n",
      "Epoch 11/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7633\n",
      "Epoch 12/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 0.7667\n",
      "Epoch 13/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 0.7664\n",
      "Epoch 14/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 0.7691\n",
      "Epoch 15/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 0.7691\n",
      "Epoch 16/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7683\n",
      "Epoch 17/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 0.7746\n",
      "Epoch 18/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 0.7764\n",
      "Epoch 19/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 0.7796\n",
      "Epoch 20/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7750\n",
      "Epoch 21/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7699\n",
      "Epoch 22/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7744\n",
      "Epoch 23/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7738\n",
      "Epoch 24/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7708\n",
      "Epoch 25/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 0.7829\n",
      "Epoch 26/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7706\n",
      "Epoch 27/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7735\n",
      "Epoch 28/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7804\n",
      "Epoch 29/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7766\n",
      "Epoch 30/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7743\n",
      "Epoch 31/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7852\n",
      "Epoch 32/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7825\n",
      "Epoch 33/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 0.7935\n",
      "Epoch 34/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 0.7822\n",
      "Epoch 35/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 0.7818\n",
      "Epoch 36/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7788\n",
      "Epoch 37/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7769\n",
      "Epoch 38/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 0.7798\n",
      "Epoch 39/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 0.7797\n",
      "Epoch 40/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 0.7781\n",
      "Epoch 41/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7858\n",
      "Epoch 42/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7941\n",
      "Epoch 43/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7829\n",
      "Epoch 44/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 0.7836\n",
      "Epoch 45/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 0.7818\n",
      "Epoch 46/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7850\n",
      "Epoch 47/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7730\n",
      "Epoch 48/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7849\n",
      "Epoch 49/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 0.7949\n",
      "Epoch 50/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 0.7764\n",
      "Epoch 51/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 0.7738\n",
      "Epoch 52/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 0.7818\n",
      "Epoch 53/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7823\n",
      "Epoch 54/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7799\n",
      "Epoch 55/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 0.7809\n",
      "Epoch 56/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 0.7826\n",
      "Epoch 57/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7826\n",
      "Epoch 58/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7955\n",
      "Epoch 59/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7888\n",
      "Epoch 60/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7816\n",
      "Epoch 61/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7713\n",
      "Epoch 62/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 0.7840\n",
      "Epoch 63/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 0.7812\n",
      "Epoch 64/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7827\n",
      "Epoch 65/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7791\n",
      "Epoch 66/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 0.7869\n",
      "Epoch 67/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 0.7935\n",
      "Epoch 68/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 0.7838\n",
      "Epoch 69/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7799\n",
      "Epoch 70/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7769\n",
      "Epoch 71/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7780\n",
      "Epoch 72/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7820\n",
      "Epoch 73/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7828\n",
      "Epoch 74/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 0.7758\n",
      "Epoch 75/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7779\n",
      "Epoch 76/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7795\n",
      "Epoch 77/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7813\n",
      "Epoch 78/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7834\n",
      "Epoch 79/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 0.7865\n",
      "Epoch 80/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7785\n",
      "62/62 [==============================] - 0s 862us/step - loss: 0.8736\n",
      "Mean squared error for activation relu: 0.87\n",
      "Epoch 1/80\n",
      "186/186 [==============================] - 1s 2ms/step - loss: 536.8238\n",
      "Epoch 2/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 511.7614\n",
      "Epoch 3/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 487.9456\n",
      "Epoch 4/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 466.7117\n",
      "Epoch 5/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 446.7734\n",
      "Epoch 6/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 426.5953\n",
      "Epoch 7/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 406.3827\n",
      "Epoch 8/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 387.8940\n",
      "Epoch 9/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 371.1699\n",
      "Epoch 10/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 355.7099\n",
      "Epoch 11/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 341.1811\n",
      "Epoch 12/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 327.3802\n",
      "Epoch 13/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 314.1772\n",
      "Epoch 14/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 301.4841\n",
      "Epoch 15/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 289.2403\n",
      "Epoch 16/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 277.4048\n",
      "Epoch 17/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 265.9465\n",
      "Epoch 18/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 254.8371\n",
      "Epoch 19/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 244.0538\n",
      "Epoch 20/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 233.5832\n",
      "Epoch 21/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 223.4154\n",
      "Epoch 22/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 213.5343\n",
      "Epoch 23/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 203.9385\n",
      "Epoch 24/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 194.6092\n",
      "Epoch 25/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 185.5538\n",
      "Epoch 26/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 176.7553\n",
      "Epoch 27/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 168.2165\n",
      "Epoch 28/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 159.9328\n",
      "Epoch 29/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 151.8970\n",
      "Epoch 30/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 144.1071\n",
      "Epoch 31/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 136.5640\n",
      "Epoch 32/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 129.2556\n",
      "Epoch 33/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 122.1853\n",
      "Epoch 34/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 115.3553\n",
      "Epoch 35/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 108.7580\n",
      "Epoch 36/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 102.3951\n",
      "Epoch 37/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 96.2600\n",
      "Epoch 38/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 90.3523\n",
      "Epoch 39/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 84.6695\n",
      "Epoch 40/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 79.2150\n",
      "Epoch 41/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 73.9785\n",
      "Epoch 42/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 68.9626\n",
      "Epoch 43/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 64.1593\n",
      "Epoch 44/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 59.5715\n",
      "Epoch 45/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 55.2002\n",
      "Epoch 46/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 51.0360\n",
      "Epoch 47/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 47.0840\n",
      "Epoch 48/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 43.3356\n",
      "Epoch 49/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 39.7894\n",
      "Epoch 50/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 36.4457\n",
      "Epoch 51/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 33.2947\n",
      "Epoch 52/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 30.3409\n",
      "Epoch 53/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 27.5749\n",
      "Epoch 54/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 24.9985\n",
      "Epoch 55/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 22.6015\n",
      "Epoch 56/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 20.3821\n",
      "Epoch 57/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 18.3389\n",
      "Epoch 58/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 16.4630\n",
      "Epoch 59/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 14.7519\n",
      "Epoch 60/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 13.1979\n",
      "Epoch 61/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 11.7989\n",
      "Epoch 62/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 10.5438\n",
      "Epoch 63/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 9.4281\n",
      "Epoch 64/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 8.4461\n",
      "Epoch 65/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 7.5902\n",
      "Epoch 66/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 6.8506\n",
      "Epoch 67/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 6.2191\n",
      "Epoch 68/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 5.6897\n",
      "Epoch 69/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 5.2497\n",
      "Epoch 70/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 4.8911\n",
      "Epoch 71/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 4.6040\n",
      "Epoch 72/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 4.3795\n",
      "Epoch 73/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 4.2083\n",
      "Epoch 74/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 4.0822\n",
      "Epoch 75/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 3.9916\n",
      "Epoch 76/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 3.9285\n",
      "Epoch 77/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 3.8858\n",
      "Epoch 78/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 3.8592\n",
      "Epoch 79/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 3.8424\n",
      "Epoch 80/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 3.8322\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 5.3265\n",
      "Mean squared error for activation softmax: 5.33\n",
      "Epoch 1/80\n",
      "186/186 [==============================] - 1s 1ms/step - loss: 470.3174\n",
      "Epoch 2/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 391.3162\n",
      "Epoch 3/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 343.0260\n",
      "Epoch 4/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 303.2060\n",
      "Epoch 5/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 267.8677\n",
      "Epoch 6/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 236.0389\n",
      "Epoch 7/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 207.2468\n",
      "Epoch 8/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 181.1923\n",
      "Epoch 9/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 157.6480\n",
      "Epoch 10/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 136.4283\n",
      "Epoch 11/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 117.3769\n",
      "Epoch 12/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 100.3545\n",
      "Epoch 13/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 85.2095\n",
      "Epoch 14/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 71.8067\n",
      "Epoch 15/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 60.0439\n",
      "Epoch 16/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 49.8008\n",
      "Epoch 17/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 40.9575\n",
      "Epoch 18/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 33.4024\n",
      "Epoch 19/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 27.0251\n",
      "Epoch 20/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 21.7128\n",
      "Epoch 21/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 17.3565\n",
      "Epoch 22/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 13.8407\n",
      "Epoch 23/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 11.0623\n",
      "Epoch 24/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 8.9156\n",
      "Epoch 25/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 7.2996\n",
      "Epoch 26/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 6.1206\n",
      "Epoch 27/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 5.2837\n",
      "Epoch 28/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 4.7157\n",
      "Epoch 29/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 4.3429\n",
      "Epoch 30/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 4.1093\n",
      "Epoch 31/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 3.9727\n",
      "Epoch 32/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 3.8955\n",
      "Epoch 33/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 3.8558\n",
      "Epoch 34/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 3.8357\n",
      "Epoch 35/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 3.8274\n",
      "Epoch 36/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "186/186 [==============================] - 0s 2ms/step - loss: 3.8241\n",
      "Epoch 37/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 3.8233\n",
      "Epoch 38/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 3.8226\n",
      "Epoch 39/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 3.8226\n",
      "Epoch 40/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 3.8225\n",
      "Epoch 41/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 3.8226\n",
      "Epoch 42/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 3.8225\n",
      "Epoch 43/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 3.8226\n",
      "Epoch 44/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 3.8227\n",
      "Epoch 45/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 3.8226\n",
      "Epoch 46/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 3.8226\n",
      "Epoch 47/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 3.8230\n",
      "Epoch 48/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 3.8227\n",
      "Epoch 49/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 3.8230\n",
      "Epoch 50/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 3.8232\n",
      "Epoch 51/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 3.8227\n",
      "Epoch 52/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 3.8224\n",
      "Epoch 53/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 3.8227\n",
      "Epoch 54/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 3.8228\n",
      "Epoch 55/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 3.8194\n",
      "Epoch 56/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 2.8937\n",
      "Epoch 57/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 1.2887\n",
      "Epoch 58/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 1.0467\n",
      "Epoch 59/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 0.9891\n",
      "Epoch 60/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.8965\n",
      "Epoch 61/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.8453\n",
      "Epoch 62/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.8267\n",
      "Epoch 63/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.8181\n",
      "Epoch 64/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.8116\n",
      "Epoch 65/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.8093\n",
      "Epoch 66/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7971\n",
      "Epoch 67/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 0.7893\n",
      "Epoch 68/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7789\n",
      "Epoch 69/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 0.7716\n",
      "Epoch 70/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 0.7682\n",
      "Epoch 71/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 0.7642\n",
      "Epoch 72/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 0.7603\n",
      "Epoch 73/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7619\n",
      "Epoch 74/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 0.7604\n",
      "Epoch 75/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7584\n",
      "Epoch 76/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 0.7588\n",
      "Epoch 77/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 0.7574\n",
      "Epoch 78/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 0.7580\n",
      "Epoch 79/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7585\n",
      "Epoch 80/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 0.7582\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.8083\n",
      "Mean squared error for activation sigmoid: 0.81\n",
      "Epoch 1/80\n",
      "186/186 [==============================] - 1s 2ms/step - loss: 186.3675\n",
      "Epoch 2/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 1.2088\n",
      "Epoch 3/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7669\n",
      "Epoch 4/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7565\n",
      "Epoch 5/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7579\n",
      "Epoch 6/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7579\n",
      "Epoch 7/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 0.7579\n",
      "Epoch 8/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7569\n",
      "Epoch 9/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7579\n",
      "Epoch 10/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 0.7627\n",
      "Epoch 11/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7602\n",
      "Epoch 12/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7603\n",
      "Epoch 13/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7596\n",
      "Epoch 14/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7628\n",
      "Epoch 15/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7637\n",
      "Epoch 16/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7652\n",
      "Epoch 17/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7628\n",
      "Epoch 18/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7641\n",
      "Epoch 19/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7686\n",
      "Epoch 20/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7671\n",
      "Epoch 21/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7713\n",
      "Epoch 22/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7683\n",
      "Epoch 23/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7702\n",
      "Epoch 24/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7743\n",
      "Epoch 25/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 0.7705\n",
      "Epoch 26/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7701\n",
      "Epoch 27/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 0.7696\n",
      "Epoch 28/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 0.7800\n",
      "Epoch 29/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 0.7712\n",
      "Epoch 30/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 0.7749\n",
      "Epoch 31/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7749\n",
      "Epoch 32/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 0.7765\n",
      "Epoch 33/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 0.7714\n",
      "Epoch 34/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 0.7708\n",
      "Epoch 35/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7744\n",
      "Epoch 36/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7710\n",
      "Epoch 37/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 0.7785\n",
      "Epoch 38/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7699\n",
      "Epoch 39/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7768\n",
      "Epoch 40/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 0.7784\n",
      "Epoch 41/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7854\n",
      "Epoch 42/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7798\n",
      "Epoch 43/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 0.7755\n",
      "Epoch 44/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7794\n",
      "Epoch 45/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7728\n",
      "Epoch 46/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 0.7691\n",
      "Epoch 47/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7705\n",
      "Epoch 48/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7873\n",
      "Epoch 49/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 0.7760\n",
      "Epoch 50/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7778\n",
      "Epoch 51/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7669\n",
      "Epoch 52/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 0.7740\n",
      "Epoch 53/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7723\n",
      "Epoch 54/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 0.7721\n",
      "Epoch 55/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7707\n",
      "Epoch 56/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7752\n",
      "Epoch 57/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7744\n",
      "Epoch 58/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7753\n",
      "Epoch 59/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 0.7739\n",
      "Epoch 60/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 0.7754\n",
      "Epoch 61/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 0.7758\n",
      "Epoch 62/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 0.7780\n",
      "Epoch 63/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 0.7705\n",
      "Epoch 64/80\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 0.7719\n",
      "Epoch 65/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7694\n",
      "Epoch 66/80\n",
      "186/186 [==============================] - 0s 2ms/step - loss: 0.7698\n",
      "Epoch 67/80\n",
      "  1/186 [..............................] - ETA: 0s - loss: 1.6322"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_1096\\1632184700.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'relu'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'softmax'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'sigmoid'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'selu'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'tanh'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m     \u001b[0mactivations\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_1096\\1632184700.py\u001b[0m in \u001b[0;36mactivations\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mmodel6\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'adam'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'mse'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mmodel6\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_scaled1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m80\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel6\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test_scaled1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1639\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1640\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcatch_stop_iteration\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1641\u001b[1;33m                     \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1642\u001b[0m                         with tf.profiler.experimental.Trace(\n\u001b[0;32m   1643\u001b[0m                             \u001b[1;34m\"train\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36msteps\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1369\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_insufficient_data\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Set by `catch_stop_iteration`.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1370\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1371\u001b[1;33m             \u001b[0moriginal_spe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_steps_per_execution\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1372\u001b[0m             can_run_full_execution = (\n\u001b[0;32m   1373\u001b[0m                 \u001b[0moriginal_spe\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    637\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    638\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 639\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    640\u001b[0m     raise NotImplementedError(\n\u001b[0;32m    641\u001b[0m         \"numpy() is only available when eager execution is enabled.\")\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py\u001b[0m in \u001b[0;36mread_value\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    725\u001b[0m     \"\"\"\n\u001b[0;32m    726\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Read\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m       \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_read_variable_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m     \u001b[1;31m# Return an identity so it can get placed on whatever device the context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    729\u001b[0m     \u001b[1;31m# specifies instead of the device where the variable is.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py\u001b[0m in \u001b[0;36m_read_variable_op\u001b[1;34m(self, no_copy)\u001b[0m\n\u001b[0;32m    704\u001b[0m           \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread_and_set_handle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mno_copy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    705\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 706\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread_and_set_handle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mno_copy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    707\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    708\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py\u001b[0m in \u001b[0;36mread_and_set_handle\u001b[1;34m(no_copy)\u001b[0m\n\u001b[0;32m    694\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mno_copy\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mforward_compat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward_compatible\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2022\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    695\u001b[0m         \u001b[0mgen_resource_variable_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdisable_copy_on_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 696\u001b[1;33m       result = gen_resource_variable_ops.read_variable_op(\n\u001b[0m\u001b[0;32m    697\u001b[0m           self.handle, self._dtype)\n\u001b[0;32m    698\u001b[0m       \u001b[0m_maybe_set_handle_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_resource_variable_ops.py\u001b[0m in \u001b[0;36mread_variable_op\u001b[1;34m(resource, dtype, name)\u001b[0m\n\u001b[0;32m    578\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    579\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 580\u001b[1;33m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[0;32m    581\u001b[0m         _ctx, \"ReadVariableOp\", name, resource, \"dtype\", dtype)\n\u001b[0;32m    582\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def activations(x):\n",
    "    model6 = keras.Sequential([\n",
    "        keras.layers.Dense(32, input_shape=(14,), activation=x),\n",
    "        keras.layers.Dense(16, activation=x),\n",
    "        keras.layers.Dense(8, activation=x),\n",
    "        keras.layers.Dense(1)\n",
    "\n",
    "    ])\n",
    "    \n",
    "    model6.compile(optimizer='adam', loss='mse')\n",
    "    \n",
    "    model6.fit(X_train_scaled1, y_train, epochs=80)\n",
    "    \n",
    "    loss = model6.evaluate(X_test_scaled1, y_test)\n",
    "    print(f'Mean squared error for activation {x}: {loss:.2f}')\n",
    "\n",
    "for i in ['relu', 'softmax', 'sigmoid', 'selu', 'tanh']:\n",
    "    activations(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "867b013ee5a64e27aaab1267e2c5c740",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 171437,
    "execution_start": 1672112184751,
    "source_hash": "dea6da3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(Dense(128, input_dim=10, kernel_initializer='he_uniform', activation='relu'))\n",
    "model2.add(Dense(64, activation='relu'))\n",
    "model2.add(Dense(14, activation='sigmoid'))\n",
    "model2.summary()\n",
    "model2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "fit = model.fit(X_transf, y, epochs=250,  verbose=1, validation_split=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "cell_id": "c69b353cfffa4dd29baeb69b337a47f0",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 177035,
    "execution_start": 1672120702849,
    "source_hash": "e783f1aa",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_8 (Dense)             (None, 128)               1408      \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 14)                910       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 10,574\n",
      "Trainable params: 10,574\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/250\n",
      "245/245 [==============================] - 0s 1ms/step - loss: 501.9124 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 2/250\n",
      "245/245 [==============================] - 0s 1ms/step - loss: 501.9125 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 3/250\n",
      "245/245 [==============================] - 0s 2ms/step - loss: 501.9124 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 4/250\n",
      "245/245 [==============================] - 0s 1ms/step - loss: 501.9124 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 5/250\n",
      "245/245 [==============================] - 0s 2ms/step - loss: 501.9124 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 6/250\n",
      "245/245 [==============================] - 0s 1ms/step - loss: 501.9123 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 7/250\n",
      "245/245 [==============================] - 0s 1ms/step - loss: 501.9121 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 8/250\n",
      "245/245 [==============================] - 0s 1ms/step - loss: 501.9124 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 9/250\n",
      "245/245 [==============================] - 0s 2ms/step - loss: 501.9123 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 10/250\n",
      "245/245 [==============================] - 0s 2ms/step - loss: 501.9125 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 11/250\n",
      "245/245 [==============================] - 0s 2ms/step - loss: 501.9124 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 12/250\n",
      "245/245 [==============================] - 0s 1ms/step - loss: 501.9120 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 13/250\n",
      "245/245 [==============================] - 0s 1ms/step - loss: 501.9124 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 14/250\n",
      "245/245 [==============================] - 0s 2ms/step - loss: 501.9125 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 15/250\n",
      "245/245 [==============================] - 0s 2ms/step - loss: 501.9125 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 16/250\n",
      "175/245 [====================>.........] - ETA: 0s - loss: 501.8055 - accuracy: 0.5352"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_1884\\2445143860.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mmodel3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'categorical_crossentropy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'sgd'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mfit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_transf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m250\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1648\u001b[0m                         ):\n\u001b[0;32m   1649\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1650\u001b[1;33m                             \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1651\u001b[0m                             \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1652\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    878\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 880\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    881\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    882\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    910\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    911\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 912\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_no_variable_creation_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    913\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_variable_creation_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    914\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    132\u001b[0m       (concrete_function,\n\u001b[0;32m    133\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m--> 134\u001b[1;33m     return concrete_function._call_flat(\n\u001b[0m\u001b[0;32m    135\u001b[0m         filtered_flat_args, captured_inputs=concrete_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m    136\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1743\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1744\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1745\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1746\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    376\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    377\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 378\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    379\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    380\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     50\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     53\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     54\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model3 = Sequential()\n",
    "model3.add(Dense(128, input_dim=10, kernel_initializer='he_uniform', activation='relu'))\n",
    "model3.add(Dense(64, activation='relu'))\n",
    "model3.add(Dense(14, activation='sigmoid'))\n",
    "model3.summary()\n",
    "model3.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "\n",
    "fit = model.fit(X_transf, y, epochs=250,  verbose=1, validation_split=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "cell_id": "84dcba62eb3a49baa0c9e4f05443025d",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 202084,
    "execution_start": 1672122789518,
    "source_hash": "550077d0",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_11 (Dense)            (None, 128)               1408      \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 14)                910       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 10,574\n",
      "Trainable params: 10,574\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/250\n",
      "245/245 [==============================] - 0s 1ms/step - loss: 501.9124 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 2/250\n",
      "245/245 [==============================] - 0s 1ms/step - loss: 501.9125 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 3/250\n",
      "245/245 [==============================] - 0s 1ms/step - loss: 501.9123 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 4/250\n",
      "245/245 [==============================] - 0s 1ms/step - loss: 501.9122 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 5/250\n",
      "245/245 [==============================] - 0s 1ms/step - loss: 501.9125 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 6/250\n",
      "245/245 [==============================] - 0s 1ms/step - loss: 501.9124 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 7/250\n",
      "245/245 [==============================] - 0s 1ms/step - loss: 501.9123 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 8/250\n",
      "245/245 [==============================] - 0s 2ms/step - loss: 501.9124 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 9/250\n",
      "245/245 [==============================] - 0s 1ms/step - loss: 501.9124 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 10/250\n",
      "245/245 [==============================] - 0s 1ms/step - loss: 501.9125 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 11/250\n",
      "220/245 [=========================>....] - ETA: 0s - loss: 502.1311 - accuracy: 0.5379"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_1884\\1615749399.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mmodel4\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'categorical_crossentropy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'sgd'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mfit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_transf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m250\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1692\u001b[0m                             \u001b[0msteps_per_execution\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_steps_per_execution\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1693\u001b[0m                         )\n\u001b[1;32m-> 1694\u001b[1;33m                     val_logs = self.evaluate(\n\u001b[0m\u001b[0;32m   1695\u001b[0m                         \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_x\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1696\u001b[0m                         \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_y\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[0;32m   2038\u001b[0m                         ):\n\u001b[0;32m   2039\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_test_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2040\u001b[1;33m                             \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2041\u001b[0m                             \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2042\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    878\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 880\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    881\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    882\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    917\u001b[0m       \u001b[1;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    918\u001b[0m       \u001b[1;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 919\u001b[1;33m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_variable_creation_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    920\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_variables\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mALLOW_DYNAMIC_VARIABLE_CREATION\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    921\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    132\u001b[0m       (concrete_function,\n\u001b[0;32m    133\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m--> 134\u001b[1;33m     return concrete_function._call_flat(\n\u001b[0m\u001b[0;32m    135\u001b[0m         filtered_flat_args, captured_inputs=concrete_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m    136\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1743\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1744\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1745\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1746\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    376\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    377\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 378\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    379\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    380\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     50\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     53\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     54\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model4 = Sequential()\n",
    "model4.add(Dense(128, input_dim=10, kernel_initializer='he_uniform', activation='relu'))\n",
    "model4.add(Dense(64, activation='softmax'))\n",
    "model4.add(Dense(14, activation='sigmoid'))\n",
    "model4.summary()\n",
    "model4.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "\n",
    "fit = model.fit(X_transf, y, epochs=250,  verbose=1, validation_split=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "cell_id": "6336893994f64b40a284f21a5664630e",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 183046,
    "execution_start": 1672123064087,
    "source_hash": "3b3a1554",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_14 (Dense)            (None, 128)               1408      \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 14)                910       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 10,574\n",
      "Trainable params: 10,574\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/250\n",
      "245/245 [==============================] - 0s 1ms/step - loss: 501.9126 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 2/250\n",
      "245/245 [==============================] - 0s 2ms/step - loss: 501.9124 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 3/250\n",
      "245/245 [==============================] - 0s 2ms/step - loss: 501.9123 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 4/250\n",
      "245/245 [==============================] - 0s 2ms/step - loss: 501.9123 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 5/250\n",
      "245/245 [==============================] - 0s 2ms/step - loss: 501.9122 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 6/250\n",
      "245/245 [==============================] - 0s 2ms/step - loss: 501.9124 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 7/250\n",
      "245/245 [==============================] - 0s 1ms/step - loss: 501.9126 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 8/250\n",
      "245/245 [==============================] - 0s 2ms/step - loss: 501.9125 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 9/250\n",
      "245/245 [==============================] - 0s 2ms/step - loss: 501.9124 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 10/250\n",
      "245/245 [==============================] - 0s 2ms/step - loss: 501.9122 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 11/250\n",
      "245/245 [==============================] - 0s 2ms/step - loss: 501.9120 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 12/250\n",
      "245/245 [==============================] - 0s 2ms/step - loss: 501.9124 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 13/250\n",
      "245/245 [==============================] - 0s 2ms/step - loss: 501.9124 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 14/250\n",
      "245/245 [==============================] - 0s 2ms/step - loss: 501.9124 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 15/250\n",
      "245/245 [==============================] - 0s 2ms/step - loss: 501.9124 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 16/250\n",
      "245/245 [==============================] - 0s 2ms/step - loss: 501.9124 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 17/250\n",
      "245/245 [==============================] - 0s 2ms/step - loss: 501.9124 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 18/250\n",
      "245/245 [==============================] - 0s 1ms/step - loss: 501.9122 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 19/250\n",
      "245/245 [==============================] - 0s 2ms/step - loss: 501.9124 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 20/250\n",
      "245/245 [==============================] - 0s 2ms/step - loss: 501.9123 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 21/250\n",
      "245/245 [==============================] - 0s 2ms/step - loss: 501.9124 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 22/250\n",
      "245/245 [==============================] - 0s 2ms/step - loss: 501.9123 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 23/250\n",
      "245/245 [==============================] - 0s 1ms/step - loss: 501.9124 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 24/250\n",
      "245/245 [==============================] - 0s 2ms/step - loss: 501.9123 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 25/250\n",
      "245/245 [==============================] - 0s 2ms/step - loss: 501.9124 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 26/250\n",
      "245/245 [==============================] - 0s 1ms/step - loss: 501.9125 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 27/250\n",
      "245/245 [==============================] - 0s 1ms/step - loss: 501.9124 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 28/250\n",
      "245/245 [==============================] - 0s 2ms/step - loss: 501.9123 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 29/250\n",
      "245/245 [==============================] - 0s 2ms/step - loss: 501.9126 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 30/250\n",
      "245/245 [==============================] - 0s 2ms/step - loss: 501.9125 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 31/250\n",
      "245/245 [==============================] - 0s 1ms/step - loss: 501.9123 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 32/250\n",
      "245/245 [==============================] - 0s 2ms/step - loss: 501.9124 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 33/250\n",
      "245/245 [==============================] - 0s 2ms/step - loss: 501.9125 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 34/250\n",
      "245/245 [==============================] - 0s 1ms/step - loss: 501.9124 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 35/250\n",
      "245/245 [==============================] - 0s 2ms/step - loss: 501.9124 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 36/250\n",
      "245/245 [==============================] - 0s 1ms/step - loss: 501.9126 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 37/250\n",
      "245/245 [==============================] - 0s 1ms/step - loss: 501.9122 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 38/250\n",
      "245/245 [==============================] - 0s 1ms/step - loss: 501.9125 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 39/250\n",
      "245/245 [==============================] - 0s 1ms/step - loss: 501.9125 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 40/250\n",
      "245/245 [==============================] - 0s 1ms/step - loss: 501.9123 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 41/250\n",
      "245/245 [==============================] - 0s 1ms/step - loss: 501.9123 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 42/250\n",
      "245/245 [==============================] - 0s 1ms/step - loss: 501.9123 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 43/250\n",
      "245/245 [==============================] - 0s 1ms/step - loss: 501.9122 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 44/250\n",
      "245/245 [==============================] - 0s 1ms/step - loss: 501.9124 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 45/250\n",
      "245/245 [==============================] - 0s 1ms/step - loss: 501.9122 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 46/250\n",
      "245/245 [==============================] - 0s 1ms/step - loss: 501.9123 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 47/250\n",
      "245/245 [==============================] - 0s 1ms/step - loss: 501.9123 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 48/250\n",
      "245/245 [==============================] - 0s 1ms/step - loss: 501.9125 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 49/250\n",
      "245/245 [==============================] - 0s 1ms/step - loss: 501.9125 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 50/250\n",
      "245/245 [==============================] - 0s 2ms/step - loss: 501.9124 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/250\n",
      "245/245 [==============================] - 0s 2ms/step - loss: 501.9124 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 52/250\n",
      "245/245 [==============================] - 0s 1ms/step - loss: 501.9123 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 53/250\n",
      "245/245 [==============================] - 0s 1ms/step - loss: 501.9125 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 54/250\n",
      "245/245 [==============================] - 0s 1ms/step - loss: 501.9124 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 55/250\n",
      "245/245 [==============================] - 0s 1ms/step - loss: 501.9123 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 56/250\n",
      "245/245 [==============================] - 0s 1ms/step - loss: 501.9124 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 57/250\n",
      "245/245 [==============================] - 0s 1ms/step - loss: 501.9125 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 58/250\n",
      "245/245 [==============================] - 0s 1ms/step - loss: 501.9125 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 59/250\n",
      "245/245 [==============================] - 0s 1ms/step - loss: 501.9124 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 60/250\n",
      "245/245 [==============================] - 0s 1ms/step - loss: 501.9123 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 61/250\n",
      "245/245 [==============================] - 0s 1ms/step - loss: 501.9124 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 62/250\n",
      "245/245 [==============================] - 0s 1ms/step - loss: 501.9124 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 63/250\n",
      "245/245 [==============================] - 0s 1ms/step - loss: 501.9125 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 64/250\n",
      "245/245 [==============================] - 0s 2ms/step - loss: 501.9124 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 65/250\n",
      "245/245 [==============================] - 0s 1ms/step - loss: 501.9123 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 66/250\n",
      "245/245 [==============================] - 0s 1ms/step - loss: 501.9122 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 67/250\n",
      "245/245 [==============================] - 0s 1ms/step - loss: 501.9124 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 68/250\n",
      "245/245 [==============================] - 0s 1ms/step - loss: 501.9123 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 69/250\n",
      "245/245 [==============================] - 0s 1ms/step - loss: 501.9125 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 70/250\n",
      "245/245 [==============================] - 0s 1ms/step - loss: 501.9125 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 71/250\n",
      "245/245 [==============================] - 0s 1ms/step - loss: 501.9122 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 72/250\n",
      "245/245 [==============================] - 0s 1ms/step - loss: 501.9123 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 73/250\n",
      "245/245 [==============================] - 0s 1ms/step - loss: 501.9124 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 74/250\n",
      "245/245 [==============================] - 0s 1ms/step - loss: 501.9124 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 75/250\n",
      "245/245 [==============================] - 0s 1ms/step - loss: 501.9124 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 76/250\n",
      "245/245 [==============================] - 0s 1ms/step - loss: 501.9124 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 77/250\n",
      "245/245 [==============================] - 0s 1ms/step - loss: 501.9124 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 78/250\n",
      "245/245 [==============================] - 0s 1ms/step - loss: 501.9122 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 79/250\n",
      "245/245 [==============================] - 0s 1ms/step - loss: 501.9122 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 80/250\n",
      "245/245 [==============================] - 0s 1ms/step - loss: 501.9124 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 81/250\n",
      "245/245 [==============================] - 0s 1ms/step - loss: 501.9122 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 82/250\n",
      "245/245 [==============================] - 0s 1ms/step - loss: 501.9126 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 83/250\n",
      "245/245 [==============================] - 0s 1ms/step - loss: 501.9124 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 84/250\n",
      "245/245 [==============================] - 0s 1ms/step - loss: 501.9124 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 85/250\n",
      "245/245 [==============================] - 0s 1ms/step - loss: 501.9124 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 86/250\n",
      "245/245 [==============================] - 0s 1ms/step - loss: 501.9121 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 87/250\n",
      "245/245 [==============================] - 0s 1ms/step - loss: 501.9125 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 88/250\n",
      "245/245 [==============================] - 0s 1ms/step - loss: 501.9124 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 89/250\n",
      "245/245 [==============================] - 0s 1ms/step - loss: 501.9124 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 90/250\n",
      "245/245 [==============================] - 0s 1ms/step - loss: 501.9125 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 91/250\n",
      "245/245 [==============================] - 0s 1ms/step - loss: 501.9124 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 92/250\n",
      "245/245 [==============================] - 0s 1ms/step - loss: 501.9122 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 93/250\n",
      "245/245 [==============================] - 0s 1ms/step - loss: 501.9124 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 94/250\n",
      "245/245 [==============================] - 0s 1ms/step - loss: 501.9124 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 95/250\n",
      "245/245 [==============================] - 0s 1ms/step - loss: 501.9124 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 96/250\n",
      "245/245 [==============================] - 0s 1ms/step - loss: 501.9124 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 97/250\n",
      "245/245 [==============================] - 0s 1ms/step - loss: 501.9122 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 98/250\n",
      "245/245 [==============================] - 0s 2ms/step - loss: 501.9122 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 99/250\n",
      "245/245 [==============================] - 0s 1ms/step - loss: 501.9124 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 100/250\n",
      "245/245 [==============================] - 0s 1ms/step - loss: 501.9124 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 101/250\n",
      "245/245 [==============================] - 0s 1ms/step - loss: 501.9124 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 102/250\n",
      "245/245 [==============================] - 0s 1ms/step - loss: 501.9124 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 103/250\n",
      "245/245 [==============================] - 0s 1ms/step - loss: 501.9122 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 104/250\n",
      "245/245 [==============================] - 0s 1ms/step - loss: 501.9122 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 105/250\n",
      "245/245 [==============================] - 0s 1ms/step - loss: 501.9124 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 106/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "245/245 [==============================] - 0s 1ms/step - loss: 501.9124 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 107/250\n",
      "245/245 [==============================] - 0s 1ms/step - loss: 501.9126 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 108/250\n",
      "245/245 [==============================] - 0s 1ms/step - loss: 501.9125 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 109/250\n",
      "245/245 [==============================] - 0s 1ms/step - loss: 501.9126 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 110/250\n",
      "245/245 [==============================] - 0s 1ms/step - loss: 501.9124 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 111/250\n",
      "245/245 [==============================] - 0s 1ms/step - loss: 501.9124 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 112/250\n",
      "245/245 [==============================] - 0s 1ms/step - loss: 501.9124 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 113/250\n",
      "245/245 [==============================] - 0s 1ms/step - loss: 501.9124 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 114/250\n",
      "245/245 [==============================] - 0s 1ms/step - loss: 501.9124 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 115/250\n",
      "245/245 [==============================] - 0s 1ms/step - loss: 501.9124 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 116/250\n",
      "245/245 [==============================] - 0s 1ms/step - loss: 501.9124 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 117/250\n",
      "245/245 [==============================] - 0s 1ms/step - loss: 501.9125 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 118/250\n",
      "245/245 [==============================] - 0s 1ms/step - loss: 501.9123 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 119/250\n",
      "245/245 [==============================] - 0s 1ms/step - loss: 501.9124 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 120/250\n",
      "245/245 [==============================] - 0s 1ms/step - loss: 501.9125 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 121/250\n",
      "245/245 [==============================] - 0s 1ms/step - loss: 501.9124 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 122/250\n",
      "245/245 [==============================] - 0s 1ms/step - loss: 501.9124 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 123/250\n",
      "245/245 [==============================] - 0s 1ms/step - loss: 501.9124 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 124/250\n",
      "245/245 [==============================] - 0s 1ms/step - loss: 501.9122 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 125/250\n",
      "245/245 [==============================] - 0s 1ms/step - loss: 501.9122 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 126/250\n",
      "245/245 [==============================] - 0s 1ms/step - loss: 501.9126 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 127/250\n",
      "245/245 [==============================] - 0s 1ms/step - loss: 501.9124 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 128/250\n",
      "245/245 [==============================] - 0s 1ms/step - loss: 501.9123 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 129/250\n",
      "245/245 [==============================] - 0s 1ms/step - loss: 501.9122 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 130/250\n",
      "245/245 [==============================] - 0s 1ms/step - loss: 501.9123 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 131/250\n",
      "245/245 [==============================] - 0s 2ms/step - loss: 501.9123 - accuracy: 0.5352 - val_loss: 495.8309 - val_accuracy: 0.5791\n",
      "Epoch 132/250\n",
      "138/245 [===============>..............] - ETA: 0s - loss: 502.2725 - accuracy: 0.5403"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_1884\\1072097497.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mmodel5\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'mean squared error'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'sgd'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'mse'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mfit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_transf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m250\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1648\u001b[0m                         ):\n\u001b[0;32m   1649\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1650\u001b[1;33m                             \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1651\u001b[0m                             \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1652\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    878\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 880\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    881\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    882\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    910\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    911\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 912\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_no_variable_creation_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    913\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_variable_creation_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    914\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    132\u001b[0m       (concrete_function,\n\u001b[0;32m    133\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m--> 134\u001b[1;33m     return concrete_function._call_flat(\n\u001b[0m\u001b[0;32m    135\u001b[0m         filtered_flat_args, captured_inputs=concrete_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m    136\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1743\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1744\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1745\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1746\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    376\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    377\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 378\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    379\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    380\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     50\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     53\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     54\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model5 = Sequential()\n",
    "model5.add(Dense(128, input_dim=10, kernel_initializer='he_uniform', activation='relu'))\n",
    "model5.add(Dense(64, activation='softmax'))\n",
    "model5.add(Dense(14, activation='sigmoid'))\n",
    "model5.summary()\n",
    "model5.compile(loss='mean squared error', optimizer='sgd', metrics=['mse'])\n",
    "\n",
    "fit = model.fit(X_transf, y, epochs=250,  verbose=1, validation_split=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "deepnote": {},
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "3944942c36514faebd6e754811dff977",
  "deepnote_persisted_session": {
   "createdAt": "2022-12-26T15:33:31.933Z"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
